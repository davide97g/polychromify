{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "In this notebook I will create some deep learning models to address the colorization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import Conv2D, UpSampling2D, Dense, MaxPooling2D, BatchNormalization, Conv2DTranspose\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave, imshow\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import random\n",
    "\n",
    "import visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "dataset_path = \"../preprocessed/\"\n",
    "path = '../dataset/'\n",
    "models_path = \"../models/\"\n",
    "history_path = \"../models/history/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history,model_name):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 4))\n",
    "    ax = axes.ravel()\n",
    "\n",
    "    # accuracy\n",
    "    ax[0].plot(history['accuracy'])\n",
    "    ax[0].plot(history['val_accuracy'])\n",
    "    ax[0].set_title('model accuracy')\n",
    "    ax[0].set_ylabel('accuracy')\n",
    "    ax[0].set_xlabel('epochs')\n",
    "    ax[0].legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "    # summarize history for loss\n",
    "    ax[1].plot(history['loss'])\n",
    "    ax[1].plot(history['val_loss'])\n",
    "    ax[1].set_title('model loss')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('epochs')\n",
    "    ax[1].legend(['train', 'validation'], loc='upper right')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(model.name,fontsize=16)\n",
    "    plt.show()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_comparison(validation=True,figsize=(6,6)):    \n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = figsize\n",
    "    plt.title('Validation accuracies')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.ylim([0.45, 0.8])\n",
    "    plt.xlabel('epochs')\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for name in os.listdir(history_path):    \n",
    "        \n",
    "        history_loaded = json.load(open(history_path+name, 'r'))       \n",
    "        \n",
    "        if validation:\n",
    "            if \"-ms\" in name:                                \n",
    "                plt.plot(history_loaded['val_accuracy'])\n",
    "                models.append(name.replace(\"-ms.json\",\"\"))\n",
    "        else:\n",
    "            if not \"-ms\" in name:\n",
    "                plt.plot(history_loaded['val_accuracy'])\n",
    "                models.append(name.replace(\".json\",\"\"))\n",
    "\n",
    "    plt.legend(models, loc='upper left')\n",
    "    \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(img_resized,img_recolored, figsize=(10,5),cast=False):     \n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    ax = axes.ravel()\n",
    "\n",
    "    ax[0].imshow(img_resized.astype('uint8') if cast else img_resized)\n",
    "    ax[0].set_title(\"Resized\")\n",
    "    \n",
    "    ax[1].imshow(img_recolored)\n",
    "    ax[1].set_title(\"Recolored\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Size Problem\n",
    "The literature addresses the colorization problem with rescaled images typically of size $256\\times256$. However, due to limitated resources and time, I had to lower the number of input pixels to $128\\times128$.\n",
    "\n",
    "The difference in training time is quite large and it would be impossibile in a reasonable amount of time to test different model architecture an correctly validate some hyper-parameters.\n",
    "\n",
    "Literature image input size: $$256\\times256 = 2^8\\times2^8 = 2^{16}$$\n",
    "\n",
    "My lower image input size: $$128\\times128 = 2^7\\times2^7 = 2^{14}$$\n",
    "\n",
    "There is a difference of $4$ times between the two image resolutions and this enables me to run $4$ times the number of test in order to validate my models. Eventually, once I found the best model, I will train it with the full $256\\times256$ resolution, in order to compare to previous methods.\n",
    "\n",
    "I know it is not an optimal procedure but I had to face time, memory and computational constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 128 # this is the target size of the entire process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset sample visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_path = path+\"download/landscapes4k/\" # modify here to load images from a different folder\n",
    "images = []\n",
    "for img_path in os.listdir(visualization_path)[12:24]:    \n",
    "    \n",
    "    img_color = []\n",
    "    img = img_to_array(load_img(visualization_path+img_path))\n",
    "    \n",
    "    img = resize(img, (SIZE,SIZE))\n",
    "    images.append(img)\n",
    "    \n",
    "fig = plt.figure(figsize=(8., 8.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(4, 3),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, images):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im.astype('uint8'))\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "In this stage I load the dataset from disk and apply some transformations:\n",
    "- resize images to $SIZE \\times SIZE$\n",
    "- rescaling between -1 and 1 _(this normalization is useful to neural network training)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# creates a generator for image loading and resizing\n",
    "train = train_datagen.flow_from_directory(path+\"download/\", \n",
    "                                          target_size=(SIZE, SIZE), \n",
    "                                          batch_size=4300, \n",
    "                                          class_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from RGB to Lab space\n",
    "\"\"\"\n",
    "by iterating on each image, we convert the RGB to Lab. \n",
    "Think of LAB image as a grey image in L channel and all color info stored in A and B channels. \n",
    "The input to the network will be the L channel, so I assign L channel to X vector. \n",
    "And assign A and B to Y.\n",
    "\"\"\"\n",
    "X = []\n",
    "Y = []\n",
    "for img in tqdm(train[0]):\n",
    "    try:\n",
    "        lab = rgb2lab(img)\n",
    "        X.append(lab[:,:,0]) \n",
    "        Y.append(lab[:,:,1:] / 128) #A and B values range from -127 to 128, \n",
    "        #so we divide the values by 128 to restrict values to between -1 and 1.\n",
    "    except:\n",
    "        print('error')\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape+(1,)) # dimensions to be the same for X and Y\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split\n",
    "\n",
    "The first split separates the training and validation data from the test data 80-20\n",
    "\n",
    "```\n",
    "[Train+Validation] + [Test] = 80% + 20%\n",
    "```\n",
    "\n",
    "Then I further divide training and validation data with again 80-20 split.\n",
    "\n",
    "```\n",
    "[Train] + [Validation] = 80% + 20%\n",
    "```\n",
    "\n",
    "In the end the percentages with respect to the total are:\n",
    "- `Train = 80% x 80%  = 64%`\n",
    "- `Validation = 20% x 80% = 16%`\n",
    "- `Test = 20%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y (A+B)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,random_state=seed) # split between train+valid / test\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.2,random_state=seed) # split between train / valid\n",
    "print(\"Train\",X_train.shape, Y_train.shape)\n",
    "print(\"Valid\",X_valid.shape, Y_valid.shape)\n",
    "print(\"Test\",X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v0 - Deep Dense AutoEncoder\n",
    "\n",
    "```javascript\n",
    "[Model selection] Validation loss: 0.017920125275850296 accuracy: 0.5373364090919495\n",
    "```\n",
    "\n",
    "This model is not powerful enough to reproduce the task of colorization.\n",
    "With less than 22k trainable parameters, a simple deep autoencoder with only dense layers is not a valid option.\n",
    "\n",
    "More important, I can clearly see that this model is not suitable for this task because even with a small dataset (100 images) and lots of epochs (50) it is not able to overfit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "model = Sequential(name=(\"AE_v0\"))\n",
    "model.add(Dense(128,activation=\"relu\", input_shape=(SIZE, SIZE, 1)))\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dense(32,activation=\"relu\"))\n",
    "\n",
    "# Decoder\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(2, activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1 - Deep Conv AutoEncoder\n",
    "\n",
    "In order to increase the power of the hidden state representation I used 2D convolutional layers instead of dense layers.\n",
    "\n",
    "This autoencoder has almost the same number of filters compared to number of neurons of the previous simple dense autoencoder.\n",
    "\n",
    "```javascript\n",
    "[Model selection] Validation loss: 0.015875518321990967 accuracy: 0.6505120992660522\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "model = Sequential(name=(\"AE_v1\"))\n",
    "model.add(Conv2D(32, (2, 2), activation='relu', padding='same', input_shape=(SIZE, SIZE, 1)))\n",
    "model.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (2, 2), activation='relu', padding='same'))\n",
    "\n",
    "#Decoder\n",
    "model.add(Conv2D(64, (2,2), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, (2,2), activation='relu', padding='same'))          \n",
    "\n",
    "# output layer\n",
    "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2 - Deep Conv Dense AutoEncoder\n",
    "\n",
    "To boost even more the hidden state representation power I added a dense architecture in the middle of the autoencoder.\n",
    "\n",
    "Specifically, 3 layers for the encoder and 2 for the decoder with 128-64-32 neurons and 64-128 neurons respectively.\n",
    "```javascript\n",
    "[Model selection] Validation loss: 0.013323986902832985 accuracy: 0.6568432450294495\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "model = Sequential(name=(\"AE_v2\"))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "#Decoder\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(16, (3,3), activation='relu', padding='same'))  \n",
    "\n",
    "# output layer\n",
    "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v3 - Super Conv AE + strides + upsampling\n",
    "\n",
    "With the previous model was not able to increase the training accuracy enough, and not even generalize of course.\n",
    "\n",
    "So, I decided to add more convolutional layers and add strides of size 2 on the encoder side. In this way I want to encapsulate a local invariant strategy to produce color consistency.\n",
    "\n",
    "```javascript\n",
    "[Model selection] Validation loss: 0.014786677435040474 accuracy: 0.6420919895172119\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "model = Sequential(name=(\"AE_v3\"))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', strides=2, input_shape=(SIZE, SIZE, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same', strides=2))\n",
    "\n",
    "#Decoder\n",
    "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "# Output layer\n",
    "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "```javascript\n",
    "[Model selection] Validation loss: 0.013957465067505836 accuracy: 0.6535870432853699\n",
    "[Hyper-parameters tuning] Validation loss: 0.013996655121445656 accuracy: 0.6794722080230713\n",
    "Test loss: 0.014167173765599728 accuracy: 0.6725620627403259\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name=(\"CNN\"))\n",
    "\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 1)))    \n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))    \n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(2, (1, 1), activation='tanh', padding='valid'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECCV16\n",
    "\n",
    "In order to exactly compare my model with previous studies I translate the ECCV16 pytorch model into tensorflow framework.\n",
    "In this way I could train it exactly on the same dataset and report its performance.\n",
    "```javascript\n",
    "[Hyper-parameters tuning] Validation loss: 0.015318002551794052 accuracy: 0.6477094888687134\n",
    "[Testing] Test loss: 0.014844442717730999 accuracy: 0.6400381326675415\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name=(\"ECCV16\"))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', strides=2, padding='same'))    \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', strides=2, padding='same'))    \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))    \n",
    "model.add(Conv2D(256, (3, 3), activation='relu', strides=2, padding='same'))    \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))    \n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))    \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate=2, padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate=2, padding='same'))    \n",
    "model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate=2, padding='same'))    \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate=2, padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate=2, padding='same'))    \n",
    "model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate=2, padding='same'))    \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))    \n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))    \n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2DTranspose(256, (4, 4), activation='relu', strides=2, padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))    \n",
    "\n",
    "model.add(Conv2D(313, (1, 1), activation='relu', padding='valid'))    \n",
    "\n",
    "model.add(Conv2D(2, (1, 1), activation='tanh', padding='valid'))    \n",
    "\n",
    "model.add(UpSampling2D((4, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss='mse', metrics=['accuracy'])\n",
    "model.summary()\n",
    "visualkeras.layered_view(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for model selection\n",
    "\n",
    "Performing `model selection` I train with a reduce number of training examples and validation examples. In fact, in order to speed up the selection step I tested only a (discrete) subset of the entire dataset. In this phase I don't want to precisely tune my model but instead search for the right architecture that will be refined by the next `hyper-parameter tuning` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = 800\n",
    "valid_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train[:training_size], Y_train[:training_size],\n",
    "    validation_data=(X_valid[:valid_size], Y_valid[:valid_size]),\n",
    "    epochs=25, \n",
    "    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history.history,model.name)\n",
    "\n",
    "valid_loss, valid_acc = model.evaluate(X_valid[:valid_size], Y_valid[:valid_size], batch_size=16, verbose=0)\n",
    "\n",
    "print(f\"[Model selection] Validation loss: {valid_loss} accuracy: {valid_acc}\")\n",
    "\n",
    "json.dump(history.history, open(history_path+model.name+\"-ms.json\", 'w'))\n",
    "model.save(models_path+model.name+\"-ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall comparison for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_comparison(True,(8,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning\n",
    "\n",
    "```\n",
    "Neurons:\n",
    "    - L: 64-32-16-32-64\n",
    "    - M: 128-64-32-64-128\n",
    "    - H: 256-128-64-128-256\n",
    "    \n",
    "Filters:    \n",
    "    - L: 16-32-64-64-32-16\n",
    "    - H: 32-64-128-128-64-32\n",
    "    \n",
    "Strides : Y/N\n",
    "\n",
    "Batch Norm: Y/N\n",
    "\n",
    "Epochs: 25-50\n",
    "```\n",
    "\n",
    "```javascript\n",
    "[Hyper-parameters tuning] Validation loss: 0.013062547892332077 accuracy: 0.6814773082733154 (v1)\n",
    "[Hyper-parameters tuning] Validation loss: 0.013182152062654495 accuracy: 0.6792629957199097 (v2)\n",
    "[Hyper-parameters tuning] Validation loss: 0.012751569971442223 accuracy: 0.6851961016654968 (v3)\n",
    "[Hyper-parameters tuning] Validation loss: 0.013202700763940811 accuracy: 0.6727843880653381 (v4)\n",
    "[Hyper-parameters tuning] Validation loss: 0.014923742972314358 accuracy: 0.6527038216590881 (v5)\n",
    "\n",
    "Test loss: 0.012100797146558762 accuracy: 0.6845739483833313\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name=(\"Polychromify_v3\"))\n",
    "# Encoder\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Decoder\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss='mse', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualkeras.layered_view(model,spacing=200, scale_xy=3, scale_z=1, max_z=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), epochs=25, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history.history,model.name)\n",
    "\n",
    "valid_loss, valid_acc = model.evaluate(X_valid, Y_valid, batch_size=32, verbose=0)\n",
    "\n",
    "print(f\"[Hyper-parameters tuning] Validation loss: {valid_loss} accuracy: {valid_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model & history to disk\n",
    "json.dump(history.history, open(history_path+model.name+\".json\", 'w'))\n",
    "model.save(models_path+model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_comparison(False,(9,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model+history from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Polychromify_v3\"\n",
    "\n",
    "model = tf.keras.models.load_model(\n",
    "    models_path+model_name,\n",
    "    custom_objects=None,\n",
    "    compile=True)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history_loaded = json.load(open(history_path+model_name+\".json\", 'r'))   \n",
    "plot_history(history_loaded,model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Final model testing.\n",
    "\n",
    "> I do **not** validate on this set, the following metrics are just meant to be _reported_ and not used as indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, Y_test, batch_size=32, verbose=1)\n",
    "\n",
    "print(f\"Test loss: {test_loss} accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(X_test[:-10]):     \n",
    "    \n",
    "    img_color = []\n",
    "    img_color.append(x)\n",
    "    img_color = np.array(img_color, dtype=float)\n",
    "\n",
    "    output = model.predict(img_color)\n",
    "    output = output*128\n",
    "    \n",
    "    result = np.zeros((SIZE, SIZE, 3))\n",
    "    result[:,:,0] = img_color[0][:,:,0]\n",
    "    result[:,:,1:] = output[0]\n",
    "\n",
    "    recolored = lab2rgb(result)\n",
    "    \n",
    "    original = np.zeros((SIZE, SIZE, 3))\n",
    "    original[:,:,0] = x[:,:,0]\n",
    "    original[:,:,1:] = Y_test[i]*128\n",
    "    \n",
    "    original = lab2rgb(original)\n",
    "    \n",
    "    plot_comparison(original, recolored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"../test/\" # modify here to load images from a different folder\n",
    "\n",
    "for i, img_path in enumerate(os.listdir(test_path)):    \n",
    "    \n",
    "    img_color = []\n",
    "    img = img_to_array(load_img(test_path+img_path))\n",
    "    original_shape = img.shape\n",
    "    \n",
    "    img = resize(img, (SIZE,SIZE))\n",
    "    \n",
    "    img_color.append(img)\n",
    "    \n",
    "    img_color = np.array(img_color, dtype=float)\n",
    "    img_color = rgb2lab(1.0/255*img_color)[:,:,:,0]\n",
    "    img_color = img_color.reshape(img_color.shape+(1,))\n",
    "\n",
    "    output = model.predict(img_color)\n",
    "    output = output*128\n",
    "    \n",
    "    result = np.zeros((SIZE, SIZE, 3))\n",
    "    result[:,:,0] = img_color[0][:,:,0]\n",
    "    result[:,:,1:] = output[0]\n",
    "\n",
    "    recolored = lab2rgb(result)\n",
    "    # to resize back to original shape\n",
    "    # recolored = resize(recolored, (original_shape[0],original_shape[1]))\n",
    "    imsave(\"../results/img/\"+str(i)+\".png\", img)\n",
    "    imsave(\"../results/img/\"+str(i)+\"-polychromify.png\", recolored)\n",
    "    \n",
    "    plot_comparison(img, recolored,cast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results comparison between different methods\n",
    "> This code snippet requires that `pretrained.ipynb` is already executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "results_path = \"../results/img/\"\n",
    "\n",
    "\n",
    "for img_path in os.listdir(results_path):\n",
    "    \n",
    "    img_color = []\n",
    "    img = img_to_array(load_img(results_path+img_path))\n",
    "    \n",
    "    img = resize(img, (SIZE,SIZE))\n",
    "    images.append(img)\n",
    "    \n",
    "fig = plt.figure(figsize=(8., 16.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(5, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, (i,im) in zip(grid, enumerate(images)):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im.astype('uint8'))\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    if i == 0:\n",
    "        ax.set_title(\"ECCV16\")\n",
    "    elif i == 1:\n",
    "        ax.set_title(\"PolyChromify\")\n",
    "    elif i == 2:\n",
    "        ax.set_title(\"SIGGRAPH17\")\n",
    "    elif i == 3:\n",
    "        ax.set_title(\"Original\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
